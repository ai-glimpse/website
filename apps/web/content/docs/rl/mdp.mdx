---
title: Markov Decision Processes
description: Foundational framework for reinforcement learning
icon: Network
---

# Markov Decision Processes (MDPs)

Markov Decision Processes (MDPs) provide the mathematical framework for modeling decision-making problems where outcomes are partly random and partly under the control of a decision-maker. MDPs are the foundation of reinforcement learning.

## The Markov Property

The Markov property states that the future depends only on the current state and not on the sequence of events that preceded it. Formally:

```math
P(S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, \ldots, S_0, A_0) = P(S_{t+1} | S_t, A_t)
```

This means that the current state provides all the information needed to predict the future state, making the system "memoryless."

## Components of an MDP

An MDP is defined by a 5-tuple $(S, A, P, R, \gamma)$:

1. **S**: A set of states
2. **A**: A set of actions
3. **P**: A state transition probability function  
   $P(s' | s, a)$ is the probability of transitioning to state $s'$ when taking action $a$ in state $s$
4. **R**: A reward function  
   $R(s, a, s')$ is the reward received after transitioning from state $s$ to state $s'$ due to action $a$
5. **$\gamma$**: A discount factor $\gamma \in [0, 1]$ that trades off the importance of immediate and future rewards

## Policies

A policy $\pi$ defines the behavior of an agent. It maps states to actions (or probability distributions over actions):

- **Deterministic policy**: $\pi(s) = a$
- **Stochastic policy**: $\pi(a|s) = P(A_t = a | S_t = s)$

## Value Functions

Value functions estimate how good it is for an agent to be in a given state or to take a specific action in a state:

### State-Value Function

The state-value function $V_\pi(s)$ is the expected return starting from state $s$ and following policy $\pi$:

```math
V_\pi(s) = E_\pi [G_t | S_t = s] = E_\pi [\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s]
```

### Action-Value Function

The action-value function $Q_\pi(s, a)$ is the expected return starting from state $s$, taking action $a$, and thereafter following policy $\pi$:

```math
Q_\pi(s, a) = E_\pi [G_t | S_t = s, A_t = a] = E_\pi [\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a]
```

## Bellman Equations

The Bellman equations express the relationship between the value of a state and the values of its successor states:

### Bellman Expectation Equation for $V_\pi$

```math
V_\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V_\pi(s')]
```

### Bellman Expectation Equation for $Q_\pi$

```math
Q_\pi(s,a) = \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma \sum_{a'} \pi(a'|s') Q_\pi(s',a')]
```

## Optimal Policies and Value Functions

The goal in an MDP is to find an optimal policy $\pi_*$ that maximizes the expected return from any starting state:

```math
\pi_* = \arg\max_\pi V_\pi(s) \quad \forall s \in S
```

The optimal value functions are:

```math
V_*(s) = \max_\pi V_\pi(s) = \max_a Q_*(s,a)
```

```math
Q_*(s,a) = \max_\pi Q_\pi(s,a) = \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma \max_{a'} Q_*(s',a')]
```

## Bellman Optimality Equations

The Bellman optimality equations characterize the optimal value functions:

```math
V_*(s) = \max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V_*(s')]
```

```math
Q_*(s,a) = \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma \max_{a'} Q_*(s',a')]
```

## Simple MDP Example: Grid World

Let's consider a simple grid world where:
- States are cells in a grid
- Actions are movements (up, down, left, right)
- Transitions are deterministic (except at boundaries)
- Rewards are -1 for each step, +10 for reaching the goal, and -10 for falling into a pit

```python
import numpy as np

# Define the grid world
# 0: Empty cell, 1: Goal, -1: Pit
grid = np.array([
    [0, 0, 0, 1],
    [0, -1, 0, -1],
    [0, 0, 0, 0]
])

# Define actions
# 0: Up, 1: Right, 2: Down, 3: Left
actions = [(0, -1), (1, 0), (0, 1), (-1, 0)]

# Define parameters
gamma = 0.9  # Discount factor
theta = 1e-6  # Convergence threshold

# Initialize value function
V = np.zeros_like(grid, dtype=float)

# Value Iteration
def value_iteration(grid, V, gamma, theta):
    while True:
        delta = 0
        for i in range(grid.shape[0]):
            for j in range(grid.shape[1]):
                if grid[i, j] != 0:  # Skip terminal states
                    continue
                
                v = V[i, j]
                values = []
                
                for action in actions:
                    next_i, next_j = i + action[0], j + action[1]
                    
                    # Check if the action leads outside the grid
                    if next_i < 0 or next_i >= grid.shape[0] or next_j < 0 or next_j >= grid.shape[1]:
                        next_i, next_j = i, j  # Stay in the same place
                    
                    reward = -1  # Step cost
                    if grid[next_i, next_j] == 1:  # Goal
                        reward = 10
                    elif grid[next_i, next_j] == -1:  # Pit
                        reward = -10
                    
                    values.append(reward + gamma * V[next_i, next_j])
                
                V[i, j] = max(values)
                delta = max(delta, abs(v - V[i, j]))
        
        if delta < theta:
            break
    
    return V

# Run value iteration
V = value_iteration(grid, V, gamma, theta)
print("Optimal Value Function:")
print(V)
```

## Variants of MDPs

- **Partially Observable MDPs (POMDPs)**: The agent cannot observe the full state directly
- **Infinite-horizon MDPs**: Decision processes that continue indefinitely
- **Continuous MDPs**: MDPs with continuous state and/or action spaces

## Applications of MDPs

MDPs are used in many domains:
- Robotics and control systems
- Game playing
- Resource management
- Healthcare decision-making
- Finance and economics
- Network routing

In the next section, we'll explore Q-learning, a model-free reinforcement learning algorithm for solving MDPs. 